"use strict";(self.webpackChunkcarol_soares_blog=self.webpackChunkcarol_soares_blog||[]).push([[7119],{5144:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"large-language-models-llm","metadata":{"permalink":"/carol-soares-blog/blog/large-language-models-llm","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-05-31-about-llm.md","source":"@site/blog/2025-05-31-about-llm.md","title":"What is a Large Language Model (LLM)?","description":"A simple yet technical guide to understanding Large Language Models (LLMs) and how they\u2019re reshaping the way we build products.","date":"2025-05-31T00:00:00.000Z","tags":[{"inline":true,"label":"ai","permalink":"/carol-soares-blog/blog/tags/ai"},{"inline":true,"label":"llm","permalink":"/carol-soares-blog/blog/tags/llm"},{"inline":true,"label":"generative ai","permalink":"/carol-soares-blog/blog/tags/generative-ai"},{"inline":true,"label":"product-development","permalink":"/carol-soares-blog/blog/tags/product-development"}],"readingTime":5.07,"hasTruncateMarker":false,"authors":[{"name":"Carol Soares","title":"Backend Enginner @ Thoughtworks","url":"https://github.com/carolinesoarws","page":{"permalink":"/carol-soares-blog/blog/authors/carolsoares"},"socials":{"linkedin":"https://www.linkedin.com/in/caroline-soares-0ba238107/","github":"https://github.com/carolinesoarws"},"imageURL":"https://avatars.githubusercontent.com/u/83501330?v=4","key":"carolsoares"}],"frontMatter":{"slug":"large-language-models-llm","title":"What is a Large Language Model (LLM)?","authors":"carolsoares","tags":["ai","llm","generative ai","product-development"],"description":"A simple yet technical guide to understanding Large Language Models (LLMs) and how they\u2019re reshaping the way we build products."},"unlisted":false,"nextItem":{"title":"My Experience at TheDevConf 2025","permalink":"/carol-soares-blog/blog/dtc-conference"}},"content":"Hey everyone! \ud83d\udcbb\u2728 <br />\\n\\nToday I want to talk about something I\u2019ve been working: **Large Language Models** (LLMs). If you\'ve been hearing a lot about ChatGPT, Bard, Claude, or LLaMA and wondered *what all the buzz is about* \u2014 I\u2019ve got you. Let\u2019s break it down in a clear and friendly way.\\n\\n\\n## What Exactly Is a Large Language Model?\\n\\nA **Large Language Model** is a type of **artificial intelligence** designed to understand and generate human language. Think of it as a super smart assistant that\u2019s read massive amounts of data \u2014 books, articles, code, forum posts, and even images or videos \u2014 and now knows how to generate coherent and useful text responses. \ud83e\udd2f\\n\\nThey\'re called *\\"large\\"* because they contain billions (and sometimes trillions!) of parameters \u2014 which are essentially learned settings that help the model detect patterns in language.\\n\\n\\n## How Are LLMs Created?\\n\\nEverything starts with **training** \u2014 and here\u2019s where things get a little technical, but in a good way.\\n\\nAt the core of most LLMs is a deep learning architecture called the **Transformer**, introduced by Google in 2017. (For the curious minds, here\u2019s the original [\u201cAttention Is All You Need\u201d](https://arxiv.org/abs/1706.03762) paper.) This architecture revolutionized the way we work with language in AI by making it possible to model relationships between words over long distances in a text.\\n\\nRather than analyzing words one at a time, Transformers allow the model to understand entire sequences at once \u2014 similar to how *you* read a sentence and instantly grasp its meaning, even without emojis or GIFs. \ud83d\udcac\\n\\n### Tools That Power LLM Development\\n\\nMost LLMs are developed using **Python**, alongside popular deep learning libraries like:\\n\\n* **PyTorch** \u2013 widely used in research and production\\n* **TensorFlow** \u2013 known for its scalability and performance\\n* **JAX** \u2013 favored by Google for high-performance model training\\n\\nThese frameworks help build the model\u2019s internal architecture, which includes:\\n\\n* **Embedding layers** \u2013 turn words into vector representations the model can process\\n* **Multi-head self-attention** \u2013 lets the model \\"pay attention\\" to all words in a sentence at once\\n* **Feed-forward neural networks** \u2013 deepen the understanding of context\\n* **Layer normalization and residual connections** \u2013 stabilize the learning process\\n\\nThese components are stacked dozens of times (up to 96 layers in models like GPT-4), resulting in deep networks with *massive* capacity for language understanding.\\n\\n\\n## How Do LLMs Actually Learn?\\n\\nLLMs are trained using **self-supervised learning**, which means they learn by predicting words in vast amounts of unlabeled text.\\n\\nThere are two common objectives:\\n\\n* **Causal Language Modeling (CLM)** \u2013 Predict the *next* word in a sentence (e.g., GPT)\\n* **Masked Language Modeling (MLM)** \u2013 Predict *missing* words in a sentence (e.g., BERT)\\n\\nTo learn, the model uses:\\n\\n* **Backpropagation** \u2013 a mathematical technique for adjusting internal parameters based on mistakes\\n* **Gradient descent** \u2013 an optimization method that helps the model improve gradually with each example\\n\\nWith time and repetition, the model becomes excellent at predicting what comes next in a sentence \u2014 which is exactly what allows it to write stories, generate code, answer questions, and so much more.\\n\\n\\n## Training at Scale: Think Brain Meets Gym\\n\\nTraining an LLM is like putting a super brain in a data gym \u2014 for *weeks* or even *months*.\\n\\nIt requires:\\n\\n* **Hardware**: Hundreds to thousands of GPUs (e.g., NVIDIA A100s) or TPUs\\n* **Data**: Terabytes of curated text \u2014 books, articles, forums, code repositories, etc.\\n* **Optimization techniques**:\\n\\n  * **Mixed precision training** (for memory efficiency)\\n  * **Gradient checkpointing** (to reduce memory usage)\\n  * **Data/model parallelism** (to spread the workload across multiple devices)\\n\\n\\n## What Happens After Training?\\n\\nOnce the model is trained, it can be:\\n\\n* **Fine-tuned** for specific domains (e.g., healthcare, law, customer support)\\n* **Aligned** with human values using **Reinforcement Learning from Human Feedback (RLHF)** \u2014 the secret sauce behind ChatGPT\u2019s natural tone\\n\\n\\n## Where Are LLMs Being Used?\\n\\nEverywhere! Some real-world applications include:\\n\\n* **Chatbots** and **virtual assistants** (like ChatGPT, Bard, Copilot)\\n* **Internal knowledge bases** and **customer support tools**\\n* **Coding assistants** (GitHub Copilot, Cody, etc.)\\n* **Business intelligence** (summarizing reports, extracting insights)\\n* **Education** (tutoring, quiz generation)\\n* **Marketing & eCommerce** (product descriptions, reviews, emails)\\n\\n\\n## How Developers Use LLMs Every Day\\n\\nLLMs are easier to integrate than ever \u2014 here\u2019s how devs can use them right now:\\n\\n### 1. **OpenAI GPT (ChatGPT)**\\n\\nUse the [OpenAI API](https://platform.openai.com/docs) to add `gpt-4` or `gpt-3.5-turbo` to your apps.\\n\\nUse cases:\\nChatbots, AI helpdesks, content generation, code explanation, test generation.\\n\\n\\n### 2. **Google PaLM 2 / Gemini**\\n\\nGoogle Cloud\u2019s [Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/overview) supports powerful LLMs for enterprise apps.\\n\\nUse cases:\\nMultimodal apps, enterprise search, document analysis.\\n\\n\\n### 3. **Anthropic Claude**\\n\\n[Claude](https://www.anthropic.com/index/introducing-claude) is known for its thoughtful responses and safe design. Great for structured Q\\\\&A and summarization.\\n\\n\\n### 4. **Meta LLaMA (Open Source)**\\n\\n[Meta\u2019s LLaMA](https://ai.meta.com/llama/) models are open-source and ideal for:\\n\\n* **Privacy-first apps** \u2013 Your users\' data stays with you\\n* **On-device AI** \u2013 No internet? No problem\\n* **Lightweight experimentation** \u2013 Great for prototyping without cloud costs\\n\\n\\n### 5. **Hugging Face Transformers**\\n\\nThe ultimate playground for developers and researchers.\\n[HuggingFace Transformers](https://huggingface.co/docs/transformers/index) lets you load and test LLMs like GPT, T5, BERT, and more:\\n\\n```bash\\npip install transformers\\n```\\n\\n```python\\nfrom transformers import pipeline\\n\\ngenerator = pipeline(\\"text-generation\\", model=\\"gpt2\\")\\nprint(generator(\\"Once upon a time,\\", max_length=50))\\n```\\n\\n\\n## LLMs in Products: Real Examples\\n\\n* **E-commerce**: Smart descriptions, review summaries, product Q\\\\&A\\n* **EdTech**: Lesson planners, tutoring, content feedback\\n* **Healthcare**: Summarizing notes, answering medical FAQs (responsibly)\\n* **Business**: Analyzing emails, reports, and meeting summaries\\n\\n---\\n\\n## \u2764\ufe0f Final Thoughts\\n\\nWe\u2019re only scratching the surface of what LLMs can do. As developers, product builders, and creators, we\u2019re in a moment of incredible opportunity.\\n\\n\u2728 Start small \u2014 test an API, build a chatbot, experiment with a prompt.\\nYou\u2019ll be surprised how far your creativity can take you with these tools at your side.\\n\\n---\\n\\n## Useful Links Recap\\n\\n* [OpenAI GPT API Docs](https://platform.openai.com/docs)\\n* [HuggingFace Transformers](https://huggingface.co/docs/transformers/index)\\n* [Google Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/overview)\\n* [Anthropic Claude](https://www.anthropic.com/index/introducing-claude)\\n* [Meta LLaMA](https://ai.meta.com/llama/)\\n* [\u201cAttention is All You Need\u201d (original paper)](https://arxiv.org/abs/1706.03762)\\n* [Large language model \u2013 Wikipedia](https://en.wikipedia.org/wiki/Large_language_model)"},{"id":"dtc-conference","metadata":{"permalink":"/carol-soares-blog/blog/dtc-conference","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-04-23-tdc-conference.md","source":"@site/blog/2025-04-23-tdc-conference.md","title":"My Experience at TheDevConf 2025","description":"Hey everyone! \ud83d\udcbb\u2728","date":"2025-04-23T00:00:00.000Z","tags":[{"inline":true,"label":"generative AI","permalink":"/carol-soares-blog/blog/tags/generative-ai"},{"inline":true,"label":"innovation","permalink":"/carol-soares-blog/blog/tags/innovation"},{"inline":true,"label":"learnings","permalink":"/carol-soares-blog/blog/tags/learnings"},{"inline":true,"label":"artificial intelligence","permalink":"/carol-soares-blog/blog/tags/artificial-intelligence"},{"inline":true,"label":"technology","permalink":"/carol-soares-blog/blog/tags/technology"},{"inline":true,"label":"future","permalink":"/carol-soares-blog/blog/tags/future"},{"inline":true,"label":"creativity","permalink":"/carol-soares-blog/blog/tags/creativity"},{"inline":true,"label":"research","permalink":"/carol-soares-blog/blog/tags/research"},{"inline":true,"label":"trends","permalink":"/carol-soares-blog/blog/tags/trends"}],"readingTime":4.015,"hasTruncateMarker":false,"authors":[{"name":"Carol Soares","title":"Backend Enginner @ Thoughtworks","url":"https://github.com/carolinesoarws","page":{"permalink":"/carol-soares-blog/blog/authors/carolsoares"},"socials":{"linkedin":"https://www.linkedin.com/in/caroline-soares-0ba238107/","github":"https://github.com/carolinesoarws"},"imageURL":"https://avatars.githubusercontent.com/u/83501330?v=4","key":"carolsoares"}],"frontMatter":{"slug":"dtc-conference","title":"My Experience at TheDevConf 2025","authors":"carolsoares","tags":["generative AI","innovation","learnings","artificial intelligence","technology","future","creativity","research","trends"]},"unlisted":false,"prevItem":{"title":"What is a Large Language Model (LLM)?","permalink":"/carol-soares-blog/blog/large-language-models-llm"},"nextItem":{"title":"6 Months in an Innovation Lab","permalink":"/carol-soares-blog/blog/long-blog-post"}},"content":"Hey everyone! \ud83d\udcbb\u2728 <br />\\n\\nIt\u2019s been about 9 months since I started working on developing solutions using artificial intelligence \u2014 with a particular focus on generative AI. In recognition of my contributions, my company awarded me the opportunity to attend TDC \u2014 The Developer Conference AI, that happen in S\xe3o Paulo, Brazil.\\n\\nThis conference is aimed at developers connecting with each other, discovering new trends, discussing best practices, sharing knowledge, etc. It was my first time attending a conference, and honestly, I felt like Sheldon! \ud83d\ude02\ud83d\ude02\\n\\n<p align=\\"center\\">\\n![Sheldon Cooper, a famous character from the TV sitcom The Big Bang Theory, wearing a red t-shirt, holding chopsticks and a plate, turning his head, and smiling with his mouth closed.](../static/img/sheldon.gif)\\n</p>\\n\\n\\nI\u2019m someone who, on the surface, seems sociable, but I hate being in large crowds and never really got excited about these types of events. I\'ve always thought they were overrated. I don\u2019t even believe in astrology, but I\u2019ll blame it on my sign today hahahaha.\\n\\nThe conference took place on March 26th and 27th, 2025, and it focused on artificial intelligence. There were several panels covering various topics related to generative AI. In general, the panels discussed how AI is being applied in different areas like healthcare, agriculture, software development, everyday life, and more. And I\u2019d love to share some of the themes that impacted me the most.\\n\\n<p align=\\"center\\">\\n![a hand holding tdc name tag card](../static/img/IMG_7760.jpeg)\\n</p>\\n\\nThe first panel that really struck me was from a company that uses AI to monitor neonatal ICUs. Through the electrodes placed on the babies, data is collected and analyzed in real time by a system using generative AI. The case presented involved a newborn with complications. When the baby started having significant changes that required medical intervention, the system had already identified the issue *hours in advance* \u2014 the baby was having an epileptic seizure. With this analysis, doctors were able to act quickly, and the baby survived. In fact, the baby is now healthy and growing well, under medical supervision of course. Incredible, right? I was deeply moved by this case!\\n\\nThe second impactful panel was about the use of generative AI in agriculture. They showed how AI is transforming the industry through soil, weather, and livestock analysis... The presenter even showed AI-powered cameras being used for these analyses, for example. One thing I was curious about was the stereotype that people in agriculture are resistant to technology. The speaker explained that the new generation in agribusiness is much more open to using technology in their daily work, and even many from older generations understand its importance now. It was really cool!!\\n\\nThe third panel that impressed me was a presentation by a team working on modernizing legacy code. With the help of generative AI, a parte of the code that was expected to take a year to be complete, actually took 6 months. Amazing, right? That\u2019s a huge time savings!\\n\\nWhat I enjoyed most about the talks I participated, was that the speakers emphasized the importance of using AI consciously and responsibly. And this goes way beyond just using AI for anything\u2014it\u2019s about knowing why you\u2019re using it, applying prompt engineering to get accurate responses, validating whether the AI\'s response is correct, checking if it\u2019s \\"hallucinating,\\" and so on. Not to mention the environmental impact of AI usage, which is a real can of worms\u2014girl, it\u2019s scary!\\n\\nThis conference was amazing! I had so much fun, learned a lot, and met some incredible professionals! For example, I participated in a mentorship focused on working on international projects, which is exactly the direction I want my career to go right now. I absolutely loved it and I think the experience is totally worth it!\\n\\nNegative points:\\n - The location wasn\u2019t great, and there weren\u2019t any nearby restaurants to have lunch without a long walk\u2014which I didn\u2019t love.\\n - The food experience was poor. The event had announced that there would be partner food vendors, but in the end, there weren\u2019t any. There was a communication failure, which was a big deal since I spent the whole day there. Especially because I was alone and didn\u2019t feel safe walking around that part of S\xe3o Paulo on my own.\\n\\nPositive points:\\n - The venue itself was really nice.\\n - I got a bunch of free goodies, which was super fun!\\n\\n<p align=\\"center\\">\\n![ecobag that i got in the conference, the ecobag is white](../static/img/IMG_7758.jpeg)\\n</p>\\n\\n - The \\"silent presentation\\" format was the coolest part: basically, three panels were happening at the same time. We wore headphones and could switch stations at any time to follow whichever presentation we wanted.\\n - I really enjoyed the networking, the spaces, the mentoring sessions, and the companies that were there!\\n\\nAll in all, it was absolutely worth it!"},{"id":"long-blog-post","metadata":{"permalink":"/carol-soares-blog/blog/long-blog-post","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-03-11-long-blog-post.md","source":"@site/blog/2025-03-11-long-blog-post.md","title":"6 Months in an Innovation Lab","description":"Hey everyone! \ud83d\udcbb\u2728","date":"2025-03-11T00:00:00.000Z","tags":[{"inline":true,"label":"generative AI","permalink":"/carol-soares-blog/blog/tags/generative-ai"},{"inline":true,"label":"innovation","permalink":"/carol-soares-blog/blog/tags/innovation"},{"inline":true,"label":"learnings","permalink":"/carol-soares-blog/blog/tags/learnings"},{"inline":true,"label":"artificial intelligence","permalink":"/carol-soares-blog/blog/tags/artificial-intelligence"},{"inline":true,"label":"machine learning","permalink":"/carol-soares-blog/blog/tags/machine-learning"},{"inline":true,"label":"technology","permalink":"/carol-soares-blog/blog/tags/technology"},{"inline":true,"label":"future","permalink":"/carol-soares-blog/blog/tags/future"},{"inline":true,"label":"creativity","permalink":"/carol-soares-blog/blog/tags/creativity"},{"inline":true,"label":"trends","permalink":"/carol-soares-blog/blog/tags/trends"}],"readingTime":4.605,"hasTruncateMarker":false,"authors":[{"name":"Carol Soares","title":"Backend Enginner @ Thoughtworks","url":"https://github.com/carolinesoarws","page":{"permalink":"/carol-soares-blog/blog/authors/carolsoares"},"socials":{"linkedin":"https://www.linkedin.com/in/caroline-soares-0ba238107/","github":"https://github.com/carolinesoarws"},"imageURL":"https://avatars.githubusercontent.com/u/83501330?v=4","key":"carolsoares"}],"frontMatter":{"slug":"long-blog-post","title":"6 Months in an Innovation Lab","authors":"carolsoares","tags":["generative AI","innovation","learnings","artificial intelligence","machine learning","technology","future","creativity","trends"]},"unlisted":false,"prevItem":{"title":"My Experience at TheDevConf 2025","permalink":"/carol-soares-blog/blog/dtc-conference"}},"content":"Hey everyone! \ud83d\udcbb\u2728 <br />\\n\\nSix months ago, I started working on an innovation project. The main idea of this project is to test innovative solutions for problems using Generative AI. It\'s been a completely new world for me, as until then, I had only worked on product delivery. Working in this lab completely took me out of my bubble, especially because I am a backend engineer. During this period, I\'ve learned a few things:\\n\\n### Not Everything Can Be Solved with Generative AIs\\nThe tech world is as crazy as the release of a new pop diva\'s album. The revolution of some emerging technologies makes many people in the market, especially business leaders, rush to implement these solutions at any cost. When a pop diva releases an iconic album, it\'s literally written in the stars that all brands, trends, and even the fashion industry will ride that wave. The same happens in technology: when truly impactful innovation arises, everyone wants to adopt, adapt, and capitalize on it. The problem is that, often, this rush happens without a real understanding of whether that tool is truly the right solution for the problem.\\n\\nWhat I love most about this project is that it forces me to think outside of my bubble and even question whether the solution I\u2019m proposing really makes sense for the business problem we\u2019re solving or if Generative AI meets our expectations. Not every problem needs Generative AI to be solved, but the fact that we reflect on the effectiveness of these technologies in our challenges completely changes the idea of \\"just using technology at any cost.\\"\\n\\n### Best Practices and Automation, Yes, Always!\\nIn this project, we deliver rapid prototypes. Therefore, we are constantly testing new solutions and making them available for evaluation. The fact that we deliver only prototypes **does not** mean that software best practices should be neglected. I\u2019ve felt firsthand how important it is to apply good development practices and have automated deployment in all projects.\\n\\nUsing new technology doesn\u2019t mean we need to abandon best practices or leave them only partially structured. They exist for a good reason: to make the developer\'s life easier and increase delivery speed. It has been very satisfying to see the greatness of these practices applied in my daily work, especially aligned with Generative AI, which means that a new technology *should not override* everything else. I love software best practices, and once you work by following this model, you don\'t want to do things any other way.\\n\\n### The Exponential Power of Generative AI\\nDuring this time, I had the chance to use some Generative AI models for our deliveries, and it\u2019s fascinating to see the exponential power of these technologies. One of the most striking models was OpenAI\u2019s embedding model, as well as the model for working with voice.\\n\\nAt one point, I had to work with RAG on a vector database, combined with an embedding model. It was **soo** cool to see the program\'s behavior when returning data based on search similarity, everything felt magical! But if there\u2019s no good database foundation, things can backfire. Also, some libraries can be heavy in Python, so if you don\u2019t have a machine that can handle it, you might end up with some headaches.\\n\\nIn my experience, I used chromadb as a vector database, and wow, that was a sentimental little database. Just one wrong configuration, and it doesn\'t work properly. At the time, I had to deploy it on AWS, and even though I configured communication ports for it to access the internet, it would pick any port except the one we wanted, and that was a pain! But for small tests, it behaved well, which makes me excited about the possibilities. I\u2019m currently exploring other vector databases, but it was really cool to learn from chromadb. I didn\u2019t know the concepts too deeply, but it was super interesting to work with.\\n\\nWhat saved me many times was the fact that I had a powerful machine to work with. I\u2019ve crashed my PC about 4 times in a row testing some extremely heavy models and libraries, but my machine survived. Even working with AWS Lambda or EC2, the process of using the libraries can be slow depending on the installation process, and Docker was our ally in that fight. One thing that makes me reflect on Gen AI is the fact that we need to have a decent machine to work with some libraries and models, at least locally. At least in my experience, after we included Docker, it became much easier to work with these heavier libraries like langchain, a dream on a summer night, literally.\\n\\nWith the voice model, it was very interesting to notice the naturalness in the speech, especially with some accents, which really surprised me!! The model we tested for conversation flow had such a great tone that can be calibrated, which opens infinite possibilities for accessibility, and that\'s amazing!\\n\\nGenerative AI, when aligned correctly, is amazing and takes delivery standards to the next level! I\u2019ve learned to be flexible in this regard, the ability to test different models, aligned with best practices, is out of this world.\\n\\n### Women in Power!\\nIn this project, many amazing, intelligent, and supportive people have passed through. But I\u2019m very proud to say that the team is mostly female. What a joy it is to work with other women, share experiences\u2026 I feel heard, welcomed, and embraced. A privilege!\\n\\nThese past few months have been great."}]}}')}}]);